\chapter{Model description}
\label{Chapter2}
%-------------------------------------------------------------------------------
%----EXPONENTIAL SMOOTHING------------------------------------------------------
%-------------------------------------------------------------------------------

\section{Exponential smoothing}

\subsection{Model description}

Developed independently by Robert G. Brown and Charles C. Holt in the early 1950s to develop a tracking model for fire-control information and forecasting spare parts, exponential smoothing methods quickly became a useful technique for extrapolating serial data.

In this thesis the notation and taxonomy implemented by \citet{gardner2006exponential}, \citet{hyndman2002state} and \citet{taylor2003exponential} is used. This thesis uses three different models presented below. The following notation is used:

\begin{table}[ht]
	\renewcommand{\arraystretch}{1.4}
	\begin{tabular}{ll}
		Symbol         & Definition \\ \hline \hline
		$\alpha$       & Smoothing parameter for the level of the series. \\
		$\gamma$       & Smoothing parameter for the trend.\\
		$m$            & Number of periods in the forecast.\\
		$\phi$         & Autoregressive or damping parameter. \\
		$S_t$          & Smoothed level of the series, computed after $X_t$ is observed. \\
		$X_t$          & Observed value of the time series in period $t$. \\
		$T_t$          & Smoothed additive trend at the end of period $t$. \\
		$\hat{X}_t(m)$ & Forecast for $m$ periods ahead from origin $t$.\\
		$e_t$          & One-step-ahead forecast error, $e_t = X_t - \hat{X}_{t-1}$.\\
		& $e_t(m)$ should be used for other forecast origins.
	\end{tabular}
\caption[Notation for exponential smoothing]{Notation used to describe exponential smoothing models (same as in \citet{gardner2006exponential})}
\end{table}

\subsection*{No trend, no seasonality}

This type is the simple exponential smoothing method by \citet{brown1962smoothing}. Following \citet{gardner2006exponential} there will be two separate equations for each model, one using the recursive form and the other being the error correction form.

Given a series $\{X_t\}$ with $t = \{1,\dots,T\}$, the simple exponential smoothing model in recursive form is given by

\begin{equation}
	\label{ses}
	\begin{array}{rl}
		S_{t}&=\alpha X_{t}+(1-\alpha) S_{t-1} \\
		\hat{X}_{t}(m)&=S_{t}
	\end{array}
\end{equation}
%
while the error correction form is given by

\begin{equation}
	\begin{array}{rl}
		S_{t}&=S_{t-1}+\alpha e_{t} \\
		\hat{X}_{t}(m)&=S_{t}.
	\end{array}
\end{equation}

\subsection*{Additive trend, no seasonality}

The model with additive trend is that of \citet{holt1957forecasting} (Holt's linear method), adding a trend term to the estimated parameter one time step prior in \ref{ses}, resulting in a linear trend. The recursive form is given by

\begin{equation}
	\label{holt}
	\begin{array}{rl}
		S_{t}&=\alpha X_{t}+(1-\alpha)\left(S_{t-1}+T_{t-1}\right) \\
		T_{t}&=\gamma\left(S_{t}-S_{t-1}\right)+(1-\gamma) T_{t-1} \\
		\hat{X}_{t}(m)&=S_{t}+m T_{t}
	\end{array}
\end{equation}
%
and the error correction form by

\begin{equation}
	\begin{array}{rl}
		S_{t}&=S_{t-1}+T_{t-1}+\alpha e_{t} \\
		T_{t}&=T_{t-1}+\alpha \gamma e_{t} \\
		\hat{X}_{t}(m)&=S_{t}+m T_{t}.
	\end{array}
\end{equation}

\subsection*{Damped-additive trend, no seasonality}

In order to allow the trend to decay over time, \citet{gardner1989note} used a dampening factor, reducing the trend influence over the course of the forecast horizon $m$. The recursive form is given by 

\begin{equation}
	\label{gardner}
	\begin{array}{rl}
		S_{t}&=S_{t-1}+\phi T_{t-1}+\alpha e_{t} \\
		T_{t}&=\phi T_{t-1}+\alpha \gamma e_{t} \\
		\hat{X}_{t}(m)&=S_{t}+\sum_{i=1}^{m} \phi^{i} T_{t}
	\end{array}
\end{equation}
%
and the error correction form by

\begin{equation}
	\begin{array}{rl}
		S_{t}&=S_{t-1} R_{t-1}+\alpha e_{t} \\
		R_{t}&=R_{t-1}+\alpha \gamma e_{t} / S_{t-1} \\
		\hat{X}_{t}(m)&=S_{t} R_{t}^{m}.
	\end{array}
\end{equation}

For all above equations it is assumed for $\alpha, \gamma \in [0,1]$ otherwise observations would gain influence the further they are away from the forecast. This becomes evident once we look at the expanded form of \ref{ses}. We substitute the expression of $S_{t-1}$ back into itself and thus arrive at the geometric progression 

\begin{equation*}
	\begin{aligned}
		S_{t} &=\alpha X_{t}+(1-\alpha) S_{t-1} \\
		&=\alpha X_{t}+\alpha(1-\alpha) X_{t-1}+(1-\alpha)^{2} S_{t-2} \\
		&=\alpha\left[X_{t}+(1-\alpha) X_{t-1}+(1-\alpha)^{2} X_{t-2}+\cdots+(1-\alpha)^{t-1} X_{1}\right]+(1-\alpha)^{t} X_{0} .
	\end{aligned}
\end{equation*}
%
These expressions can be defined analogously for \ref{holt} and \ref{gardner}. For $\phi$, different values can be used to give the trend convex, linear or even concave shape. 

\subsection{Model selection}

Selecting an appropriate model can be done in several different ways, of which a few will be presented here. Depending on the number of time series to forecast either aggregate or individual model selection can be used. \citet{fildes2001beyond} comes to the conclusion that in aggregate selection, the damped-additive trend model is hard to beat, although individual selection does yield better results sometimes. The \enquote{individual selection of exponential smoothing methods, [however], is best described as inconclusive.} \citep[][p. 28]{gardner2006exponential}.

On the one hand there are selection criteria based on time-series characteristics, as described for example in \citet{shah1997model} or \citet{meade2000evidence}, which led to promising results when applied to time series that were generated using one of the processes to identify, but when applied to other series the results were less convincing. On the other hand there are expert-systems that generate rules based on experience made from earlier forecasting procedures (see for example \citet{collopy1992rule} or \citet{flores2000use}). 

The model selection used in this study will be based on information criteria, as they are easy to derive and readily available. The results in selecting the appropriate model, however, are not convincing either. Comparing the studies of \citet{gardner1985forecasting} and \citet{hyndman2002state}, one can find that only for a forecast horizon of two and 15, the AIC as an information criterion selected more accurate models rather than the aggregate choice of a damped-additive trend model. For this reason this study also fits a damped-additive trend model to the data regardless of the decision based on information criteria. Nevertheless does the choice of an information criterion as a model selector provide an easily accessible procedure as this study estimates parameters via state-space maximum likelihood, from which arbitrary information criteria can be derived handily. The choice and description of information criteria used in this study will be discussed in more detail in section SECCCTIONN!!!!!

\subsection{The state-space model}

In order to estimate the parameters for the above described models, this study uses an "innovations", single-source of error (SSOE) state-space model. The model framework is that of \citet{ord1997estimation}, which was expanded by \citet{hyndman2002state}. The basic state space framework can be described by the following equations:

\begin{subequations}
	\begin{align}
		y_{t}&=w\left(\boldsymbol{X}_{t-1}\right)+r\left(\boldsymbol{X}_{t-1}\right) \varepsilon_{t} \label{measurement} \\
		\boldsymbol{x}_{t}&=f\left(\boldsymbol{X}_{t-1}\right)+g\left(\boldsymbol{X}_{t-1}\right) \varepsilon_{t} \label{transition}	
	\end{align}
\end{subequations}
%
with $y_t$ being the observation at time $t$, $\boldsymbol{X}_t$ the state vector containing unobserved components that describe the level, trend and seasonality of the series and $w,r,f,g$ are continuous functions with $w,r: \mathbb{R}^p \rightarrow \mathbb{R}$ and $f,g: \mathbb{R} \rightarrow \mathbb{R}$. $\{\varepsilon_{t}\}$ is a Gaussian white noise process with variance $\sigma^2$. Equation \ref{measurement} is called the \textit{measurement equation} as it measures the relationship between the unobserved states $\boldsymbol{X}_{t-1}$ and the observation $y_t$. Equation \ref{transition} is called \textit{transition equation}, describing the evolution of the states over time. 

All of the equations from \ref{ses} to \ref{gardner} can be translated into state-space terminology. To ensure that this thesis is self-contained I will present the equations in their state-space equations below, however they are exactly taken from \citet{hyndman2002state}, which discusses them in great detail. The models in this study use additive trends, such that $r(\boldsymbol{X}_{t-1}) = 1$ and if we define the one-step forecast made in period $t-1$ as $\mu_t = F_{(t-1)+1} = w\left(\boldsymbol{X}_{t-1}\right)$. Further defining $e_{t}=r\left(X_{t-1}\right) \varepsilon_{t}$ we can rewrite $Y_t = \mu_t + e_t$ which is equal to $Y_t = \mu_t + \varepsilon_t$ for additive errors. Suppose that $l_t$ is the level of our series at time $t$, $b_t$ is the slope of the series at time $t$ and $\alpha, \gamma, \phi$ are constants, Brown's method from \ref{ses} becomes

\begin{equation}
	\begin{array}{rl}
		\mu_{t}&=l_{t-1} \\
		l_{t}&=l_{t-1}+\alpha \varepsilon_{t}.
	\end{array}
\end{equation}
%
Holt's linear trend method becomes

\begin{equation}
	\begin{array}{rl}
		\mu_{t}&=l_{t-1}+b_{t-1} \\
		l_{t}&=l_{t-1}+b_{t-1}+\alpha \varepsilon_{t} \\
		b_{t}&=b_{t-1}+\alpha \gamma \varepsilon_{t},
	\end{array}
\end{equation}
%
and Gardener's damped additive trend becomes

\begin{equation}
	\begin{array}{rl}
		\mu_{t}&=l_{t-1}+b_{t-1} \\
		l_{t}&=l_{t-1}+b_{t-1}+\alpha \varepsilon_{t} \\
		b_{t}&=\phi b_{t-1}+\alpha \gamma \varepsilon_{t}.
	\end{array}
\end{equation}
%

\subsection{Parameter estimation and initial values}

As we assumed the error term $\{\varepsilon_t\}$ to be Gaussian noise, the likelihood function will also be a Gaussian likelihood. In essence, the joint
density of the series is the weighted product of the densities of the individual innovations:

\begin{equation}
	p\left(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{X}_{0}, \sigma^{2}\right)=\prod_{t=1}^{n} p\left(\varepsilon_{t}\right) /\left|r\left(\boldsymbol{X}_{t-1}\right)\right|,
\end{equation}
%
with $\boldsymbol{X}_0$ being the seed states or initial values. This given, the Gaussian likelihood can be described as

\begin{equation}
\label{expsm-likeli}
	\mathcal{L}\left(\boldsymbol{\theta}, \boldsymbol{X}_{0}, \sigma^{2} \mid \boldsymbol{y}\right)=\left(2 \pi \sigma^{2}\right)^{-n / 2}\left|\prod_{t=1}^{n} r\left(\boldsymbol{X}_{t-1}\right)\right|^{-1} \exp \left(-\frac{1}{2} \sum_{t=1}^{n} \varepsilon_{t}^{2} / \sigma^{2}\right),
\end{equation}
%
and the log likelihood is given as 

\begin{equation}
	\log \mathcal{L}=-\frac{n}{2} \log \left(2 \pi \sigma^{2}\right)-\sum_{t=1}^{n} \log \left|r\left(\boldsymbol{X}_{t-1}\right)\right|-\frac{1}{2} \sum_{t=1}^{n} \varepsilon_{t}^{2} / \sigma^{2}.
\end{equation}
%
If we now set the partial derivative with respect to $\sigma^2$ to zero, we can calculate the maximum likelihood estimate of the variance as

\begin{equation}
	\hat{\sigma}^{2}=n^{-1} \sum_{t=1}^{n} \varepsilon_{t}^{2}.
\end{equation}
%
This allows us to write the likelihood from \ref{expsm-likeli} without dependency from the variance as 

\begin{equation}
	\mathcal{L}\left(\boldsymbol{\theta}, \boldsymbol{X}_{0} \mid \boldsymbol{y}\right)=\left(2 \pi  \hat{\sigma}^{2}\right)^{-n / 2}\left|\prod_{t=1}^{n} r\left(\boldsymbol{x}_{t-1}\right)\right|^{-1}.
\end{equation}
%
Recapitulating from above we know that for additive models $r(\boldsymbol{X}_{t-1}) = 1$, and thus by taking the twice the negative logarithm we arrive at

\begin{equation}
	\label{max-llik}
	\begin{array}{rl}
	\mathcal{L}^{*}\left(\boldsymbol{\theta}, \boldsymbol{X}_{0}\right)&= n \log \left(2 \pi \right) + n \log \left( \hat{\sigma}^2	 \right) \\
	&= n \log \left(2 \pi \right) + n \log \left( \displaystyle \sum_{t=1}^{n} \varepsilon_{t}^{2}\right).
	\end{array}
\end{equation}
%
To arrive at maximum likelihood estimates for the parameters this study minimizes \ref{max-llik}.

One last step prior to fitting the model, the initial values for $\boldsymbol{X}_0$ must be set. Here we use $\alpha = \gamma = 0.5$ and $\phi = 0.9$, while for the level $l_0$ of the series we compute a linear trend of the first ten observations and use the intercept as a starting value. For the initial trend  coefficient $b_0$ we use the slope of the before computed trend.

%-------------------------------------------------------------------------------
%----ARIMA(p,d,q)---------------------------------------------------------------
%-------------------------------------------------------------------------------

\section{Autoregressive integrated moving average (ARIMA)}

\subsection{The linear filter}

The basic framework for the stochastic models presented here, are based on an idea of \citet{yule1927vii} that an observable time series $y_t$ in which past successive values are dependent on each other can be seen as a series of independent shocks $\varepsilon_t$. These shocks can be modeled as random draws from a fixed distribution with mean $\mu = 0$ and time independent variance $\sigma^2_\varepsilon$. In most cases $\varepsilon_t \sim \mathcal{N} \left(0, \sigma^2_\varepsilon \right) $. The thus generated sequence of random values $\varepsilon_t, \varepsilon_{t-1}, \varepsilon_{t-2} \dots$, is called a white noise process.

The process that transforms $\varepsilon_t$ into $y_t$ is then called a \textit{linear filter}, that takes in a weighted sum of previous shocks, such that

\begin{equation}
        \label{linear-filter}
	\begin{aligned}
		z_{t} &=\mu+\varepsilon_{t}+\zeta_{1} \varepsilon_{t-1}+\zeta_{2} \varepsilon_{t-2}+ \dots \\
		&=\mu+\zeta(\boldsymbol{B}) \varepsilon_{t}.
	\end{aligned}
\end{equation}
%
The second line of the above equation makes use of the \textit{backwards shift operator}, which is is defined by $\boldsymbol{B} y_t = y_{t-1}$ and therefore $\boldsymbol{B}^m y_t = y_{t-m}$, and is called the \textit{transfer function}, defined as

\begin{equation*}
	\zeta(\boldsymbol{B})=1+\zeta_{1} \boldsymbol{B} + \zeta_{2} \boldsymbol{B}^{2}+\dots
\end{equation*}
%
The sequence of weights $\{\zeta_j\}$ can have arbitrary lengths and determines the processes' stationarity. If the weights are absolutely summable, i.e. $\sum_{j=0}^{\infty}\left|\zeta_{j}\right|<\infty$, the filter is called stable and the underlying process is stationary. Then, $\mu$ is the mean about which the process varies; otherwise $\mu$ has no specific interpretation, besides giving information about the process level.

\subsection{The autoregressive model}

The idea behind the autoregressive model is that the current value of a series can be expressed as the result of regressing past values of the series plus a random shock onto the current one, hence the name \textit{autoregressive}. For the mathematical description we will stick with the above notation: Let $y_t, y_{t-1}, y_{t-2},\dots$ be a series with equidistant time steps and further let $\tilde{y}_t = y_t - \mu$ be the series deviation from its level or mean. Now, 

\begin{equation}
        \tilde{y}_{t}=\phi_{1} \tilde{y}_{t-1}+\phi_{2} \tilde{y}_{t-2}+\dots+\phi_{p} \tilde{y}_{t-p}+\varepsilon_{t}
\end{equation}
%
is an autoregressive process of order p (AR(p)). We can also make use of the backwards shift operator to define the transfer function of the process as 

\begin{equation}
        \phi(\boldsymbol{B})=1-\phi_{1} \boldsymbol{B}-\phi_{2} \boldsymbol{B}^{2}-\dots-\phi_{p} \boldsymbol{B}^{p}
\end{equation}
%
to arrive at the economical description of the process as

\begin{equation}
        \phi(\boldsymbol{B})\tilde{y_t} = \varepsilon_t.
\end{equation}
%
The number of estimated parameters of the model is $p + 2$, i.e. $\mu_y, \sigma^2_\varepsilon \mathrm{and } \phi_j$ with $j = 1,\dots, p$. An AR(p) process can be either stationary or non-stationary depending on the unit roots of its characteristic polynomial $\phi(\boldsymbol{B})$. If the absolute value of all roots $\phi(\boldsymbol{B}) = 0$ lie outside the unit circle, the process is stationary. 

\subsection{The moving average model}

As we have seen above, the autoregressive model regresses past terms of the series plus a random shock onto the current value. The same idea is adapted for the moving average model with the change that now the current value is regessed onto a linear combination of past shocks. The model can be described as 

\begin{equation}
        \tilde{y}_{t}=\varepsilon_{t}-\theta_{1} \varepsilon_{t-1}-\theta_{2} \varepsilon_{t-2}-\dots-\theta_{q} \varepsilon_{t-q},
\end{equation}
%
which is called a \textit{moving average process} of order q (MA(q)). Again using the backwards shift operator we are able to define the moving average operator as

\begin{equation*}
        \theta(\boldsymbol{B})=1-\theta_{1} \boldsymbol{B}-\theta_{2} \boldsymbol{B}^{2}-\cdots-\theta_{q} \boldsymbol{B}^{q},
\end{equation*}
%
which enables us to economically write the model as

\begin{equation}
        \tilde{y}_t = \theta (\boldsymbol{B})\varepsilon_t.
\end{equation}
%
The number of parameters to be estimated is again $q+2$ following the same logic as above.

\subsection{Autoregressive moving average models}

Both of the above models can also be combined in order to achieve greater flexibility when modeling time series. The resulting model is called \textit{autoregressive moving average}, or ARMA and is defined by 

\begin{equation}
        \tilde{y}_{t}=\phi_{1} \tilde{z}_{t-1}+\dots+\phi_{p} \tilde{z}_{t-p}+\varepsilon_{t}-\theta_{1} \varepsilon_{t-1}-\dots-\theta_{q} \varepsilon_{t-q}.
\end{equation}
%
For easier identification the model order is usually written in parantheses after the model name, like ARMA(p,q). For a more economical writing stlye, often either the summation notation

\begin{equation}
        \tilde{y}_{t}=\varepsilon_{t}+\sum_{i=1}^{p} \phi_{i} y_{t-i}+\sum_{j=1}^{q} \theta_{j} \varepsilon_{t-j}
\end{equation}
%
or the operator notation

\begin{equation}
        \phi(\boldsymbol{B}) \tilde{y}_{t}=\theta(\boldsymbol{B}) \varepsilon_{t}
\end{equation}
%
is used. The combined model now has $p+q+2$ parameters that need to be estimated from the data. 

\subsection{Achieving stationarity}

The majority of time series encounterd in the real world are not stationary, meaning they do not fluctuate around a fixed mean but much rather exibit seasonal or global trends. However, it is possible for those series to still exhibit homogeneous behavior over time when we somehow account for those changes in level. Mathematically non-stationarity occurs if one ore more roots of the processes' characteristc polynomial lie exactly on the unit circle while all others lie outside. Following \citet{box1976time}, if such a process has exactly $d$ roots on the unit circle, we can use the augmented autoregressive operator $\varphi(B)=\phi(B)(1-B)^{d}$ to transform the model back to stationarity via

\begin{equation}
        \varphi(\boldsymbol{B}) y_{t}=\phi(\boldsymbol{B})(1-\boldsymbol{B})^{d} y_{t}=\theta(\boldsymbol{B}) \varepsilon_{t}.
\end{equation}
%
So, a model that exhibits non-stationary, but homogeneous behavior can be brought back to stationarity by using the $d$-th differnece instead. We can now define some $b_{t}=(1-\boldsymbol{B})^{d} y_{t}$ to arrive at the well known ARIMA(p,d,q) model defined by 

\begin{equation}
        b_{t}=\phi_{1} b_{t-1}+\dots \phi_{p} b_{t-p}+\varepsilon_{t}-\theta_{1} \varepsilon_{t-1}-\dots-\theta_{q} \varepsilon_{t-q}
\end{equation}
%
or in summation notation

\begin{equation}
        w_{t}=\mu+\varepsilon_{t}+\sum_{i=1}^{p} \phi_{i} y_{t-i}+\sum_{j=1}^{q} \theta_{j} \varepsilon_{t-j}.
\end{equation}

\subsection{Model identification and selection}

The process of identification and selection can take place in two different manners. First, one can follow the \textit{Box Jenkins approach}, that is inspecting the autocorrelation and partial autocorrelation functions to determine the order of the underlying process and second a grid-search algorithm in combination with some information criterion can be used to search though the models' parameter space to choose the model that minimizes the criterion

\subsection*{Autocovariance and Autocorrelation function}

Once a process is stationary by means of the definition above, it features some important stochastic stabilities especially time invariant properties. In particular this means that its properties, such as the joint probability, mean or variance are unaffected by a change of time origin. Suppose a stationary stochastic process $\{y_t\}$ is observed for some period of time $y_{t_1}, y_{t_2},\dots,y_{t_m}$, then under stationarity conditions all describing moments and the joint probability remain equal if the series gets shiftet by some arbitrary integer $s$, say $y_{t_1+s}, y_{t_2+s},\dots,y_{t_m+s}$.

The time-invariant mean of a (continuous) stationarity process with a joint probability function $p(\cdot)$ is given by

\begin{equation}
        \mu=E\left[y_{t}\right]=\int_{-\infty}^{\infty} y p(y) d y
\end{equation}
%
and its variance by

\begin{equation}
        \sigma_{y}^{2}=E\left[\left(y_{t}-\mu\right)^{2}\right]=\int_{-\infty}^{\infty}(y-\mu)^{2} p(y) d y.
\end{equation}
%
As in real-world applications the series are always of finite length, the mean and variance must be estimated from the data. For a series with $N$ observations the mean is estimated by

\begin{equation}
        \bar{y}=\frac{1}{N} \sum_{t=1}^{N} y_{t}
\end{equation}
%
and the variance via

\begin{equation}
        \hat{\sigma}_{y}^{2}=\frac{1}{N} \sum_{t=1}^{N}\left(y_{t}-\bar{y}\right)^{2}.
\end{equation}
%
The stationarity from above implies that the joint probability distribution is the same for all times that are a constant value apart. It follows herefrom that also the covariance between two values $y_t$ and $y_{t+k}$ must be the same for every $t$. The separation integer $k$ is also called the \textit{lag value}, and therefore defines the autocovariance function at lag $k$ as

\begin{equation}
        \gamma_{k}=\operatorname{cov}\left[y_{t}, y_{t+k}\right]=E\left[\left(y_{t}-\mu\right)\left(y_{t+k}-\mu\right)\right].
\end{equation}
%
Equivalently, the autocorrelation function is defined as

\begin{equation}
        \rho_{k}=\frac{E\left[\left(y_{t}-\mu\right)\left(y_{t+k}-\mu\right)\right]}{\sqrt{E\left[\left(y_{t}-\mu\right)^{2}\right] E\left[\left(y_{t+k}-\mu\right)^{2}\right]}}
\end{equation}











