\chapter{Model description}
\label{Chapter2}
%-------------------------------------------------------------------------------
%----EXPONENTIAL SMOOTHING------------------------------------------------------
%-------------------------------------------------------------------------------

\section{Exponential smoothing}

\subsection{Model description}

Developed independently by Robert G. Brown and Charles C. Holt in the early 1950s to develop a tracking model for fire-control information and forecasting spare parts, exponential smoothing methods quickly became a useful technique for extrapolating serial data.

In this thesis the notation and taxonomy implemented by \citet{gardner2006exponential}, \citet{hyndman2002state} and \citet{taylor2003exponential} is used. This thesis uses three different models presented below. The following notation is used:

\begin{table}[ht]
	\renewcommand{\arraystretch}{1.4}
	\begin{tabular}{ll}
		Symbol         & Definition \\ \hline \hline
		$\alpha$       & Smoothing parameter for the level of the series. \\
		$\gamma$       & Smoothing parameter for the trend.\\
		$m$            & Number of periods in the forecast.\\
		$\phi$         & Autoregressive or damping parameter. \\
		$S_t$          & Smoothed level of the series, computed after $X_t$ is observed. \\
		$X_t$          & Observed value of the time series in period $t$. \\
		$T_t$          & Smoothed additive trend at the end of period $t$. \\
		$\hat{X}_t(m)$ & Forecast for $m$ periods ahead from origin $t$.\\
		$e_t$          & One-step-ahead forecast error, $e_t = X_t - \hat{X}_{t-1}$.\\
		& $e_t(m)$ should be used for other forecast origins.
	\end{tabular}
\caption[Notation for exponential smoothing]{Notation used to describe exponential smoothing models (same as in \citet{gardner2006exponential})}
\end{table}

\subsection*{No trend, no seasonality}

This type is the simple exponential smoothing method by \citet{brown1962smoothing}. Following \citet{gardner2006exponential} there will be two separate equations for each model, one using the recursive form and the other being the error correction form.

Given a series $\{X_t\}$ with $t = \{1,\dots,T\}$, the simple exponential smoothing model in recursive form is given by

\begin{equation}
	\label{ses}
	\begin{array}{rl}
		S_{t}&=\alpha X_{t}+(1-\alpha) S_{t-1} \\
		\hat{X}_{t}(m)&=S_{t}
	\end{array}
\end{equation}
%
while the error correction form is given by

\begin{equation}
	\begin{array}{rl}
		S_{t}&=S_{t-1}+\alpha e_{t} \\
		\hat{X}_{t}(m)&=S_{t}.
	\end{array}
\end{equation}

\subsection*{Additive trend, no seasonality}

The model with additive trend is that of \citet{holt1957forecasting} (Holt's linear method), adding a trend term to the estimated parameter one time step prior in \ref{ses}, resulting in a linear trend. The recursive form is given by

\begin{equation}
	\label{holt}
	\begin{array}{rl}
		S_{t}&=\alpha X_{t}+(1-\alpha)\left(S_{t-1}+T_{t-1}\right) \\
		T_{t}&=\gamma\left(S_{t}-S_{t-1}\right)+(1-\gamma) T_{t-1} \\
		\hat{X}_{t}(m)&=S_{t}+m T_{t}
	\end{array}
\end{equation}
%
and the error correction form by

\begin{equation}
	\begin{array}{rl}
		S_{t}&=S_{t-1}+T_{t-1}+\alpha e_{t} \\
		T_{t}&=T_{t-1}+\alpha \gamma e_{t} \\
		\hat{X}_{t}(m)&=S_{t}+m T_{t}.
	\end{array}
\end{equation}

\subsection*{Damped-additive trend, no seasonality}

In order to allow the trend to decay over time, \citet{gardner1989note} used a dampening factor, reducing the trend influence over the course of the forecast horizon $m$. The recursive form is given by 

\begin{equation}
	\label{gardner}
	\begin{array}{rl}
		S_{t}&=S_{t-1}+\phi T_{t-1}+\alpha e_{t} \\
		T_{t}&=\phi T_{t-1}+\alpha \gamma e_{t} \\
		\hat{X}_{t}(m)&=S_{t}+\sum_{i=1}^{m} \phi^{i} T_{t}
	\end{array}
\end{equation}
%
and the error correction form by

\begin{equation}
	\begin{array}{rl}
		S_{t}&=S_{t-1} R_{t-1}+\alpha e_{t} \\
		R_{t}&=R_{t-1}+\alpha \gamma e_{t} / S_{t-1} \\
		\hat{X}_{t}(m)&=S_{t} R_{t}^{m}.
	\end{array}
\end{equation}

For all above equations it is assumed for $\alpha, \gamma \in [0,1]$ otherwise observations would gain influence the further they are away from the forecast. This becomes evident once we look at the expanded form of \ref{ses}. We substitute the expression of $S_{t-1}$ back into itself and thus arrive at the geometric progression 

\begin{equation*}
	\begin{aligned}
		S_{t} &=\alpha X_{t}+(1-\alpha) S_{t-1} \\
		&=\alpha X_{t}+\alpha(1-\alpha) X_{t-1}+(1-\alpha)^{2} S_{t-2} \\
		&=\alpha\left[X_{t}+(1-\alpha) X_{t-1}+(1-\alpha)^{2} X_{t-2}+\cdots+(1-\alpha)^{t-1} X_{1}\right]+(1-\alpha)^{t} X_{0} .
	\end{aligned}
\end{equation*}
%
These expressions can be defined analogously for \ref{holt} and \ref{gardner}. For $\phi$, different values can be used to give the trend convex, linear or even concave shape. 

\subsection{Model selection}

Selecting an appropriate model can be done in several different ways, of which a few will be presented here. Depending on the number of time series to forecast either aggregate or individual model selection can be used. \citet{fildes2001beyond} comes to the conclusion that in aggregate selection, the damped-additive trend model is hard to beat, although individual selection does yield better results sometimes. The \enquote{individual selection of exponential smoothing methods, [however], is best described as inconclusive.} \citep[][p. 28]{gardner2006exponential}.

On the one hand there are selection criteria based on time-series characteristics, as described for example in \citet{shah1997model} or \citet{meade2000evidence}, which led to promising results when applied to time series that were generated using one of the processes to identify, but when applied to other series the results were less convincing. On the other hand there are expert-systems that generate rules based on experience made from earlier forecasting procedures (see for example \citet{collopy1992rule} or \citet{flores2000use}). 

The model selection used in this study will be based on information criteria, as they are easy to derive and readily available. The results in selecting the appropriate model, however, are not convincing either. Comparing the studies of \citet{gardner1985forecasting} and \citet{hyndman2002state}, one can find that only for a forecast horizon of two and 15, the AIC as an information criterion selected more accurate models rather than the aggregate choice of a damped-additive trend model. For this reason this study also fits a damped-additive trend model to the data regardless of the decision based on information criteria. Nevertheless does the choice of an information criterion as a model selector provide an easily accessible procedure as this study estimates parameters via state-space maximum likelihood, from which arbitrary information criteria can be derived handily. The choice and description of information criteria used in this study will be discussed in more detail in section SECCCTIONN!!!!!

\subsection{The state-space model}

In order to estimate the parameters for the above described models, this study uses an "innovations", single-source of error (SSOE) state-space model. The model framework is that of \citet{ord1997estimation}, which was expanded by \citet{hyndman2002state}. The basic state space framework can be described by the following equations:

\begin{subequations}
	\begin{align}
		y_{t}&=w\left(\boldsymbol{X}_{t-1}\right)+r\left(\boldsymbol{X}_{t-1}\right) \varepsilon_{t} \label{measurement} \\
		\boldsymbol{x}_{t}&=f\left(\boldsymbol{X}_{t-1}\right)+g\left(\boldsymbol{X}_{t-1}\right) \varepsilon_{t} \label{transition}	
	\end{align}
\end{subequations}
%
with $y_t$ being the observation at time $t$, $\boldsymbol{X}_t$ the state vector containing unobserved components that describe the level, trend and seasonality of the series and $w,r,f,g$ are continuous functions with $w,r: \mathbb{R}^p \rightarrow \mathbb{R}$ and $f,g: \mathbb{R} \rightarrow \mathbb{R}$. $\{\varepsilon_{t}\}$ is a Gaussian white noise process with variance $\sigma^2$. Equation \ref{measurement} is called the \textit{measurement equation} as it measures the relationship between the unobserved states $\boldsymbol{X}_{t-1}$ and the observation $y_t$. Equation \ref{transition} is called \textit{transition equation}, describing the evolution of the states over time. 

All of the equations from \ref{ses} to \ref{gardner} can be translated into state-space terminology. To ensure that this thesis is self-contained I will present the equations in their state-space equations below, however they are exactly taken from \citet{hyndman2002state}, which discusses them in great detail. The models in this study use additive trends, such that $r(\boldsymbol{X}_{t-1}) = 1$ and if we define the one-step forecast made in period $t-1$ as $\mu_t = F_{(t-1)+1} = w\left(\boldsymbol{X}_{t-1}\right)$. Further defining $e_{t}=r\left(X_{t-1}\right) \varepsilon_{t}$ we can rewrite $Y_t = \mu_t + e_t$ which is equal to $Y_t = \mu_t + \varepsilon_t$ for additive errors. Suppose that $l_t$ is the level of our series at time $t$, $b_t$ is the slope of the series at time $t$ and $\alpha, \gamma, \phi$ are constants, Brown's method from \ref{ses} becomes

\begin{equation}
	\begin{array}{rl}
		\mu_{t}&=l_{t-1} \\
		l_{t}&=l_{t-1}+\alpha \varepsilon_{t}.
	\end{array}
\end{equation}
%
Holt's linear trend method becomes

\begin{equation}
	\begin{array}{rl}
		\mu_{t}&=l_{t-1}+b_{t-1} \\
		l_{t}&=l_{t-1}+b_{t-1}+\alpha \varepsilon_{t} \\
		b_{t}&=b_{t-1}+\alpha \gamma \varepsilon_{t},
	\end{array}
\end{equation}
%
and Gardener's damped additive trend becomes

\begin{equation}
	\begin{array}{rl}
		\mu_{t}&=l_{t-1}+b_{t-1} \\
		l_{t}&=l_{t-1}+b_{t-1}+\alpha \varepsilon_{t} \\
		b_{t}&=\phi b_{t-1}+\alpha \gamma \varepsilon_{t}.
	\end{array}
\end{equation}
%

\subsection{Parameter estimation and initial values}

As we assumed the error term $\{\varepsilon_t\}$ to be Gaussian noise, the likelihood function will also be a Gaussian likelihood. In essence, the joint
density of the series is the weighted product of the densities of the individual innovations:

\begin{equation}
	p\left(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{X}_{0}, \sigma^{2}\right)=\prod_{t=1}^{n} p\left(\varepsilon_{t}\right) /\left|r\left(\boldsymbol{X}_{t-1}\right)\right|,
\end{equation}
%
with $\boldsymbol{X}_0$ being the seed states or initial values. This given, the Gaussian likelihood can be described as

\begin{equation}
\label{expsm-likeli}
	\mathcal{L}\left(\boldsymbol{\theta}, \boldsymbol{X}_{0}, \sigma^{2} \mid \boldsymbol{y}\right)=\left(2 \pi \sigma^{2}\right)^{-n / 2}\left|\prod_{t=1}^{n} r\left(\boldsymbol{X}_{t-1}\right)\right|^{-1} \exp \left(-\frac{1}{2} \sum_{t=1}^{n} \varepsilon_{t}^{2} / \sigma^{2}\right),
\end{equation}
%
and the log likelihood is given as 

\begin{equation}
	\log \mathcal{L}=-\frac{n}{2} \log \left(2 \pi \sigma^{2}\right)-\sum_{t=1}^{n} \log \left|r\left(\boldsymbol{X}_{t-1}\right)\right|-\frac{1}{2} \sum_{t=1}^{n} \varepsilon_{t}^{2} / \sigma^{2}.
\end{equation}
%
If we now set the partial derivative with respect to $\sigma^2$ to zero, we can calculate the maximum likelihood estimate of the variance as

\begin{equation}
	\hat{\sigma}^{2}=n^{-1} \sum_{t=1}^{n} \varepsilon_{t}^{2}.
\end{equation}
%
This allows us to write the likelihood from \ref{expsm-likeli} without dependency from the variance as 

\begin{equation}
	\mathcal{L}\left(\boldsymbol{\theta}, \boldsymbol{X}_{0} \mid \boldsymbol{y}\right)=\left(2 \pi  \hat{\sigma}^{2}\right)^{-n / 2}\left|\prod_{t=1}^{n} r\left(\boldsymbol{x}_{t-1}\right)\right|^{-1}.
\end{equation}
%
Recapitulating from above we know that for additive models $r(\boldsymbol{X}_{t-1}) = 1$, and thus by taking the twice the negative logarithm we arrive at

\begin{equation}
	\label{max-llik}
	\begin{array}{rl}
	\mathcal{L}^{*}\left(\boldsymbol{\theta}, \boldsymbol{X}_{0}\right)&= n \log \left(2 \pi \right) + n \log \left( \hat{\sigma}^2	 \right) \\
	&= n \log \left(2 \pi \right) + n \log \left( \displaystyle \sum_{t=1}^{n} \varepsilon_{t}^{2}\right).
	\end{array}
\end{equation}
%
To arrive at maximum likelihood estimates for the parameters this study minimizes \ref{max-llik}.

One last step prior to fitting the model, the initial values for $\boldsymbol{X}_0$ must be set. Here we use $\alpha = \gamma = 0.5$ and $\phi = 0.9$, while for the level $l_0$ of the series we compute a linear trend of the first ten observations and use the intercept as a starting value. For the initial trend  coefficient $b_0$ we use the slope of the before computed trend.

%-------------------------------------------------------------------------------
%----ARIMA(p,d,q)---------------------------------------------------------------
%-------------------------------------------------------------------------------

\section{Autoregressive integrated moving average (ARIMA)}

\subsection{The linear filter}

The basic framework for the stochastic models presented here, are based on an idea of \citet{yule1927vii} that an observable time series $y_t$ in which past successive values are dependent on each other can be seen as a series of independent shocks $\varepsilon_t$. These shocks can be modeled as random draws from a fixed distribution with mean $\mu = 0$ and time independent variance $\sigma^2_\varepsilon$. In most cases $\varepsilon_t \sim \mathcal{N} \left(0, \sigma^2_\varepsilon \right) $. The thus generated sequence of random values $\varepsilon_t, \varepsilon_{t-1}, \varepsilon_{t-2} \dots$, is called a white noise process.

The process that transforms $\varepsilon_t$ into $y_t$ is then called a \textit{linear filter}, that takes in a weighted sum of previous shocks, such that

\begin{equation}
        \label{linear-filter}
	\begin{aligned}
		z_{t} &=\mu+\varepsilon_{t}+\zeta_{1} \varepsilon_{t-1}+\zeta_{2} \varepsilon_{t-2}+ \dots \\
		&=\mu+\zeta(\boldsymbol{B}) \varepsilon_{t}.
	\end{aligned}
\end{equation}
%
The second line of the above equation makes use of the \textit{backwards shift operator}, which is is defined by $\boldsymbol{B} y_t = y_{t-1}$ and therefore $\boldsymbol{B}^m y_t = y_{t-m}$, and is called the \textit{transfer function}, defined as

\begin{equation*}
	\zeta(\boldsymbol{B})=1+\zeta_{1} \boldsymbol{B} + \zeta_{2} \boldsymbol{B}^{2}+\dots
\end{equation*}
%
The sequence of weights $\{\zeta_j\}$ can have arbitrary lengths and determines the processes' stationarity. If the weights are absolutely summable, i.e. $\sum_{j=0}^{\infty}\left|\zeta_{j}\right|<\infty$, the filter is called stable and the underlying process is stationary. Then, $\mu$ is the mean about which the process varies; otherwise $\mu$ has no specific interpretation, besides giving information about the process level.

\subsection{The autoregressive model}

The idea behind the autoregressive model is that the current value of a series can be expressed as the result of regressing past values of the series plus a random shock onto the current one, hence the name \textit{autoregressive}. For the mathematical description we will stick with the above notation: Let $y_t, y_{t-1}, y_{t-2},\dots$ be a series with equidistant time steps and further let $\tilde{y}_t = y_t - \mu$ be the series deviation from its level or mean. Now, 

\begin{equation}
        \tilde{y}_{t}=\phi_{1} \tilde{y}_{t-1}+\phi_{2} \tilde{y}_{t-2}+\dots+\phi_{p} \tilde{y}_{t-p}+\varepsilon_{t}
\end{equation}
%
is an autoregressive process of order p (AR(p)). We can also make use of the backwards shift operator to define the transfer function of the process as 

\begin{equation}
        \phi(\boldsymbol{B})=1-\phi_{1} \boldsymbol{B}-\phi_{2} \boldsymbol{B}^{2}-\dots-\phi_{p} \boldsymbol{B}^{p}
\end{equation}
%
to arrive at the economical description of the process as

\begin{equation}
        \phi(\boldsymbol{B})\tilde{y_t} = \varepsilon_t.
\end{equation}
%
The number of estimated parameters of the model is $p + 2$, i.e. $\mu_y, \sigma^2_\varepsilon \mathrm{and} \phi_j$ with $j = 1,\dots, p$. An AR(p) process can be either stationary or non-stationary depending on the unit roots of its characteristic polynomial $\phi(\boldsymbol{B})$. If the absolute value of all roots $\phi(\boldsymbol{B}) = 0$ lie outside the unit circle, the process is stationary. 

\subsection{The moving average model}

As we have seen above, the autoregressive model regresses past terms of the series plus a random shock onto the current value. The same idea is adapted for the moving average model with the change that now the current value is regessed onto a linear combination of past shocks. The model can be described as 

\begin{equation}
        \tilde{y}_{t}=\varepsilon_{t}-\theta_{1} \varepsilon_{t-1}-\theta_{2} \varepsilon_{t-2}-\dots-\theta_{q} \varepsilon_{t-q}
\end{equation}

















