
\chapter{Introduction}
\label{Chapter1}

The aim of this thesis is to survey the accuracy and practicability of different univariate forecasting methods for different stock indices around the world. The methods used in this study are split into two groups based on their inner workings. The first group encompasses models with a well-established statistical foundation i.e. Exponential state-space and ARIMA(p,d,q) models, while the latter focuses on artificial neural networks (ANNs) which can be considered non-parametric as they do not need statistical assumptions before estimation but rather derive a structure directly from the underlying data. 

The models are configured to provide a seven-day-ahead forecast which will be compared using scale invariant error metrics. The models with statistical foundation will also be compared among themselves using maximum entropy estimators or their derivatives.

The present thesis will be structured as follows. The first chapter reviews the history and literature of all here considered models. The second chapter gives an in depth explanation of those models and points out their utility in terms of univariate time-series forecasting. The third chapter presents the data to be analyzed and applies the models to the data. The fourth chapter discusses the results and concludes.

%----------------------------------------------------------------------------

\section{History of time series forecasting and literature review}

The forecasting of serially structured data, i.e. data that has a temporal component ordering it from past to present, has challenged researchers ever since the first mathematical models for describing such data have evolved. 

%----------------------------------------------------------------------------

\subsection*{Exponential Smoothing models}

More than 60 years ago, the family of exponential smoothing models emerged from the works of \citet{brown1962smoothing}, \citet{holt1957forecasting} and \citet{winters1960forecasting}, although they did not have profound statistical foundations but much rather were seen as ad-hoc techniques for extrapolating serially dependent data. Those models received not much attention among statisticians until the mid 1980s, when \citet{gardner1985exponential} (later revised as part two (see \citet{gardner2006exponential})) published a comprehensive review and classification of all known-to-date exponential smoothing models. In the same year \citet{snyder1985recursive} laid the foundation for describing exponential smoothing models as innovation state space models and gave way for most of the recent developments in this field. Today, state-space models are the state of the art for modeling exponential smoothing, especially since \citet{hyndman2002state} and \citet{taylor2003exponential} designed automatic state space frameworks for nearly all smoothing models (following their own taxonomy), which are now also natively implemented into various statistical software packages.

Although not very complex, exponential smoothing models are widely adapted in industry and commerce for inventory control or sales forecasting. Until today surprisingly accurate forecasts can be generated with such models \citep{chatfield2001new} and even out-perform more advanced models such as ARIMA \citep{hyndman2001s}.

%------------------------------------------------------------------------------

\subsection*{ARIMA models}

Inspired by the concept of a deterministic world, \citet{yule1927vii} was the first to develop the idea that time series can be seen as realizations of stochastic processes. This simple, yet novel approach laid the foundation for ample research efforts, developing many of the standard tools for time series analysis known today. After the subsequent description of autoregressive (AR) and moving average (MA) models, \citet{box1976time} consolidated and integrated the existing knowledge and formulated a three stage approach for the identification, estimation and verification of time series. This approach, the \textit{Box-Jenkins-approach}, is used until today in various applications. 

It gave way for an abundance of studies successfully implementing their procedure (at least in parts) in different fields of investigation such as forecasting of the federal funds rate \citep{hein1988forecasting}, monthly electricity consumption \citep{harris1993dynamic} or stock price prediction of the NYSE and NSE \citep{ariyo2014stock} which is closely related to this study's goal.

\subsection*{Artificial neural networks}

Neural networks have a history that dates back nearly as long as that of the above mentioned methodologies, starting with \citet{rosenblatt1957perceptron} and his \textit{Perceptron} as a first associative memory, based on the concept of neurons working in the human brain and \citet{hu1964adaptive} using neural networks specifically for forecasting purposes. The practical utility, however, remained low in the early days as the number of solvable problems using such nets were limited due to lacking computational power and mathematical restraints. 

In the subsequent decades, neural networks received only limited research attention until \citet{rumelhart1985learning} simplified training of networks by introducing the back-propagation algorithm to tackle complex learning problems with multi-layer-perceptrons (MLPs). This publication revived interest in the field and gave way for several advances, such as the proof that MLPs are able to approximate any measurable function arbitrarily well \citep{hornik1989multilayer}, the first commercial use of neural networks for handwritten zip-code recognition \citep{lecun1989backpropagation}, the emergence of convolutional neural networks \citep{lecun1995convolutional} or the invention of long-short-term memory cells (LSTM) by \citet{hochreiter1997long} to circumvent learning problems of recurrent neural networks (RNNs).

Despite new research advancements, neural networks remained hard to train and work with, shifting away research efforts to related models such as Support Vector Machines or Random Forests (see \citet{boser1992training} or \citet{ho1995random}). 

The era of deep learning began with the work of \citet{hinton2006fast}, who showed that proper weight initialization made it possible to train networks with many hidden layers (deep networks), by pre-training every layer separately at first. These advancements lead to efficient weight intitializers that did not need the layers to be pre-trained \citep{glorot2010understanding}. The same authors also highlighted the impact of activation functions on the capabilities of neural networks, leading to new research resulting in the well known rectified linear unit (ReLU) activation function and its derivatives (\citet{jarrett2009best} and \citet{nair2010rectified}). 

The insights from the last decades, paired with an stark increase in available data and computational power, led to remarkable results in multiple fields of research, for example the forecasting of retail demand (\citet{wen2017multi}, \citet{salinas2020deepar}), traffic (\citet{laptev2017time}, \citet{li2017diffusion}) or energy (\citet{dimoulkas2019neural}).

Despite the multitude of positive feedback, there are still doubts whether artificial neural networks are being oversold. There are several papers documenting ANNs being outperformed by basic random walks (see \citet{conejo2005forecasting}, or \citet{tkacz2001neural}). 




































