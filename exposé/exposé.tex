%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt, a4paper, twocolumn]{article}

\input{structure.tex} % Specifies the document structure and loads requires packages

%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

\title{Forecasting with parametric models and neural networks: a comparison (expos√©)} % The article title

\author{
	\authorstyle{Niklas Paulig\textsuperscript{1,2,3}} % Authors
	\newline\newline % Space before institutions
	\textsuperscript{1}\institution{Technical University of Dresden, Dresden, Germany}\\
	\textsuperscript{2}\institution{Mat.Nr: 4064033}\\
	\textsuperscript{3}\institution{Master VWL, Fak. Wirtschaftswissenschaften}
}


\date{\today} % Add a date here 
%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title

\thispagestyle{firstpage} % Apply the page style for the first page (no headers and footers)

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\lettrineabstract{For this master thesis, I choose 14 stock indices from around the world to compare the forecasting abilities of different parametric statistical models against the forecasting abilities of artificial neural networks (ANNs). The focus will lie on the inner workings and parameterizations of those two tools as well as accuracy and goodness-of-fit measures. The research question asks whether black-box approaches like neural networks can outperform profound statistical forecasting models.}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Idea}

When performing time-series forecasting the usual goal is not only to predict future values from a given set of data but also to get an understanding of the data generating process. Both forcasting branches I cover in this thesis do this, but very differently.

For classical time-series analysis, I will set up an exponential smoothing model based on the works of \citet{brown1962smoothing}, \citet{holt1957forecasting} and \citet{winters1960forecasting} that will act as a baseline. Various error metrics to measure out-of-sample performance will be provided, such as RMSE, MAE and MAPE. 

In a next step, the more advanced \citet{box1976time} methodology is used to fit an ARIMA model to the data and compare it to the naive exponential smoothing in terms of accuracy and robustness. As the 14 time-series are stock indices with fluctuating variances I will also specify and fit a generalized ARCH  model to the series in order to account for its volatility clustering (\citet{bollerslev1994arch} based on the work of \citet{engle1982autoregressive}).

The classic models will be compared using relative entropy estimators like the Akaike information criterion and other related estimators such as Hannan-Quinn or the Schwarz information criterion. Unfortunately due to the nature of artificial neural networks not using likelihood functions, those cannot be compared to the classical models in this way. 

The second part of the thesis will introduce artificial neural networks, that substantially differ from the classical models in the way they work and produce forecasts. The development of neural networks will be traced back to the Perceptron algorithm (\citet{rosenblatt1957perceptron}) and the idea of a neuron will be explained. 

As a benchmark for feed-forward neural networks a multi-layer perceptron (MLP) with different parametrizations is trained on the data and out-of-sample performance in comparison to classical models is discussed. In a final step, a more complex recurrent neural network (RNN) (see for example \citet{williams1989learning} or \citet{werbos1990backpropagation}), a Long Short-Term Memory (LSTM) model after \citet{hochreiter1997long} is fitted and compared in order to get a more holistic overview over the forecasting abilities of simple and more complex artificial neural networks.

\section{Structure}

The thesis structure is planned to contain three chapters. The first chapter introduces the different inference and forecast models and explains their origin, use cases and inner workings. I will thereby start with the classical models (exp. smoothing, ARIMA, GARCH) and continue with the neural networks. The last part of the first chapter will introduce the various error and performance metrics and explain their behavior. 

Chapter two introduces the data-set and firstly describes the pre-fit transformations and statistical tests and secondly the fitting process itself. The models are then compared to each other with the above-introduced out-of-sample performance metrics. 

The last chapter draws a conclusion to the question whether the rather complicated and opaque neural networks can outperform classical statistical analysis, and if they do, to what magnitude. 



%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\printbibliography[title={References}] % Print the bibliography, section title in curly brackets

%----------------------------------------------------------------------------------------

\end{document}
